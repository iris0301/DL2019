{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/iris0301/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['realiz tonight titti man ðŸ˜‚ ðŸ˜Ž' 'positive']\n",
      " ['capper' 'neutral']\n",
      " ['actin like realli want gon get fuck good get feel ðŸ˜‹' 'positive']\n",
      " ['rememb first hear song alway favorit' 'positive']\n",
      " ['don t need cupcak ken cmon' 'neutral']\n",
      " ['ikyfl ðŸ˜­' 'negative']\n",
      " ['cant go back sleep' 'negative']\n",
      " ['haha true ðŸ˜‚' 'positive']\n",
      " ['bounc shit forev knee' 'negative']\n",
      " ['btw devour blog day love back' 'positive']\n",
      " ['get forev onli god know much love mrsgarza' 'positive']\n",
      " ['alway felt should lion senat presid glad neither' 'positive']\n",
      " ['daniel tell drink more wine while alreadi kinda buzz' 'positive']\n",
      " ['don t do drug kid ðŸ¤§' 'negative']\n",
      " ['twitter get troubl parent idc sad' 'negative']\n",
      " ['don t even get fade anymor' 'negative']\n",
      " ['win t help california forest fire' 'neutral']\n",
      " ['probabl would ve die go astro fest' 'negative']\n",
      " ['can disappear no one would notic' 'neutral']\n",
      " ['funclejustin' 'neutral']]\n",
      "40870\n"
     ]
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download(\"words\")\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('emoji/emoji_final_join.json',encoding='utf-8') as f: \n",
    "    emoji_list = json.load(f)\n",
    "\n",
    "\n",
    "stop_word = [line.rstrip('\\r\\n') for line in open(\"stop_words.txt\")]\n",
    "\n",
    "    \n",
    "\n",
    "def split(word): \n",
    "    return list(word) \n",
    "\n",
    "def clean(text,emoji):\n",
    "    am = text\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", text).split()) \n",
    "    text = text.split()\n",
    "\n",
    "    newtext = [x for x in text if not x.startswith('@') and not x.startswith('https')]\n",
    "    newtext = [x for x in text if x not in stop_word]\n",
    "    newtext = [WordNetLemmatizer().lemmatize(x,'v') for x in newtext]\n",
    "    newtext = ' '.join(newtext)\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    newtext = tokenizer.tokenize(newtext)\n",
    "    #stop_words = stopwords.words('english')\n",
    "    #newtext =  [w for w in newtext if not w in stop_words]\n",
    "    #stopwords.words will delete word 'not' which is important for sentiment analysis, so we don't remove stopwords.\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    newtext = list(map(stemmer.stem, newtext))\n",
    "    emojis = split(emoji)\n",
    "    newtext = newtext + emojis\n",
    "    label = 0\n",
    "    for ele in newtext:\n",
    "        if ele =='amp':\n",
    "            label = 1\n",
    "\n",
    "    newtext = ' '.join(newtext)\n",
    "    if label == 1:\n",
    "        newtext = newtext.replace('amp','and')\n",
    "   \n",
    "    #print(newtext)\n",
    "    return newtext\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "with open('data1.csv',encoding=\"utf8\") as words_file:\n",
    "    csv_reader = csv.DictReader(words_file, delimiter = ',')\n",
    "    data = []\n",
    "    for row in csv_reader:\n",
    "        cleaned_row = []\n",
    "        clean_text = clean(row['Text'],row['Emoji'])\n",
    "        \n",
    "        if len(clean_text) == 0:\n",
    "            continue\n",
    "            \n",
    "        if float(row['Sentiment']) > 0 and int(row['Sentiment140']) == 4:\n",
    "            sentiment = 'positive'\n",
    "        elif float(row['Sentiment']) < 0 and int(row['Sentiment140']) == 0:\n",
    "            sentiment = 'negative'\n",
    "        elif float(row['Sentiment']) == 0 and int(row['Sentiment140']) == 2:\n",
    "            if float(row['Emoji_sentiment']) > 0:\n",
    "                sentiment = 'positive'\n",
    "            elif float(row['Emoji_sentiment']) < 0:\n",
    "                sentiment = 'negative'\n",
    "            else:\n",
    "                sentiment = 'neutral'\n",
    "        elif float(row['Sentiment']) == 0:\n",
    "            if int(row['Sentiment140']) == 0:\n",
    "                sentiment = 'negative'\n",
    "            if int(row['Sentiment140']) == 4:\n",
    "                sentiment = 'positive'\n",
    "        elif int(row['Sentiment140']) == 2:\n",
    "            if float(row['Sentiment']) < 0:\n",
    "                sentiment = 'negative'\n",
    "            if float(row['Sentiment']) > 0:\n",
    "                sentiment = 'positive'\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        cleaned_row.append(clean_text)\n",
    "        cleaned_row.append(sentiment)\n",
    "        #cleaned_row.append(row['Emoji'])\n",
    "        data.append(np.array(cleaned_row))\n",
    "    data = np.array(data)\n",
    "    \n",
    "print(data[:20])\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive' 'neutral' 'positive' ... 'positive' 'negative' 'negative']\n",
      "['realiz tonight titti man ðŸ˜‚ ðŸ˜Ž' 'capper'\n",
      " 'actin like realli want gon get fuck good get feel ðŸ˜‹' ...\n",
      " 'wittelsbach magic' 'holi shit ðŸ–¤' 'see mean']\n",
      "40870\n"
     ]
    }
   ],
   "source": [
    "Y = data[:,1]\n",
    "X = data[:,0]\n",
    "print(Y)\n",
    "print(X)\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(token_pattern=r'[^\\s]+')\n",
    "newx = vectorizer.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n",
      "40870\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_array = np.array(vectorizer.get_feature_names())\n",
    "print(newx[0].toarray())\n",
    "n = 5\n",
    "X_new = []\n",
    "for i in range(len(X)):\n",
    "#     tfidf_sorting = np.argsort(newx[i].toarray())\n",
    "#     tfidf_sorting=tfidf_sorting[0][::-1]\n",
    "#     top_n = tfidf_sorting[:n]\n",
    "#     X_new.append(list(top_n))\n",
    "    X_new.append(newx[i].toarray()[0])\n",
    "print(len(X_new))\n",
    "#for col in newx.nonzero()[1]:\n",
    "#    print(feature_array[col], ' - ', top_n[0, col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_dict = dict()\n",
    "# word_id = 0\n",
    "# for ele in X:\n",
    "#     ele = ele.split()\n",
    "#     for word in ele:\n",
    "#         if word not in word_dict:\n",
    "#             word_dict[word] = word_id \n",
    "#             word_id += 1       \n",
    "    \n",
    "# X_id = []\n",
    "    \n",
    "# #print(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# all_dict = []\n",
    "\n",
    "# for i in range(len(X)):\n",
    "#     print(i)\n",
    "#     wordDict = dict.fromkeys(word_dict, 0)\n",
    "#     ele = np.array(X[i].split())\n",
    "#     for word in ele:\n",
    "#         wordDict[word] += 1\n",
    "#     all_dict.append(wordDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xnew = []\n",
    "# for i in range(len(X)):\n",
    "#     ele = X[i].split()\n",
    "#     Xnew.append(ele)\n",
    "# print(Xnew[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# def computeTF(wordDict, bow):\n",
    "#     tfDict = {}\n",
    "#     bowCount = len(bow)\n",
    "#     for word, count in wordDict.items():\n",
    "#         tfDict[word] = count/float(bowCount)\n",
    "#     return tfDict\n",
    "\n",
    "# def computeIDF(docList):\n",
    "#     idfDict = {}\n",
    "#     N = len(docList)\n",
    "    \n",
    "#     idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "#     for doc in docList:\n",
    "#         for word, val in doc.items():\n",
    "#             if val > 0:\n",
    "#                 idfDict[word] += 1\n",
    "    \n",
    "#     for word, val in idfDict.items():\n",
    "#         idfDict[word] = math.log10(N / float(val))\n",
    "        \n",
    "#     return idfDict\n",
    "\n",
    "# def computeTFIDF(tfBow, idfs):\n",
    "#     tfidf = {}\n",
    "#     for word, val in tfBow.items():\n",
    "#         tfidf[word] = val*idfs[word]\n",
    "#     return tfidf\n",
    "\n",
    "\n",
    "\n",
    "# idfs = computeIDF(all_dict)\n",
    "# result = []\n",
    "# for i in range(len(all_dict)):\n",
    "#     tf = computeTF(all_dict[i], Xnew[i])\n",
    "#     tfidf = computeTFIDF(tf, idfs)\n",
    "#     output = list(tfidf.values())\n",
    "#     result.append(output)\n",
    "\n",
    "\n",
    "# document_topic_matrix = TruncatedSVD(n_components=100, random_state=0).fit_transform(result)\n",
    "\n",
    "# id_topic={}\n",
    "# a = np.argsort(document_topic_matrix, axis=1)\n",
    "# for i in range(len(document_topic_matrix)):\n",
    "#     top5 = np.argsort(document_topic_matrix[i])[-5:][::-1]\n",
    "#     id_topic[i] = list(top5)\n",
    "\n",
    "\n",
    "# X_new = list(id_topic.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30220, 24934, 34244], [5572, 34441, 11479], [1444, 12901, 34241], [25224, 27922, 11047], [7798, 6645, 16612], [14944, 34275, 34441], [5532, 27548, 3203], [13545, 30735, 34232], [4598, 16880, 11707], [8703, 4279, 5026]]\n"
     ]
    }
   ],
   "source": [
    "print(X_new[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, Y, test_size=0.1, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-80d8188de5a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdecision_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdecision_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdecision_tree_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Decision Tree:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecision_tree_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    814\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                 \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "decision_tree = DecisionTreeClassifier(random_state=0)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "decision_tree_score = decision_tree.score(X_test, y_test)\n",
    "print('Decision Tree:', decision_tree_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2625791  0.35935049 0.37807041]\n",
      "RF: 0.4930266699290433\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=200, max_depth=5,random_state=0)\n",
    "clf.fit(X_train, y_train)  \n",
    "print(clf.feature_importances_)\n",
    "score = clf.score(X_test, y_test)\n",
    "print('RF:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN: 0.43919745534621973\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=100)\n",
    "knn.fit(X_train, y_train)\n",
    "knn_score = knn.score(X_test, y_test)\n",
    "print('KNN:',knn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2)\n",
    "transformed = tsne.fit_transform(X_new[:10000])\n",
    "yy = []\n",
    "for x in Y[:10000]:\n",
    "    if x =='positive':\n",
    "        yy.append('blue')\n",
    "    if x=='negative':\n",
    "        yy.append('red')\n",
    "    if x =='neutral':\n",
    "        yy.append('yellow')\n",
    "print(len(yy))\n",
    "colors = yy\n",
    "plt.scatter(transformed[:, 0], transformed[:, 1],c=colors)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iris0301/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-2f0666c361a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msvm_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SVM:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msvm_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(random_state=0)\n",
    "svm.fit(X_train, y_train)\n",
    "svm_score = svm.score(X_test, y_test)\n",
    "print('SVM:',svm_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "tokenized_train = [t.split() for t in X]\n",
    "phrases = Phrases(tokenized_train)\n",
    "bigram = Phraser(phrases)\n",
    "tg_phrases = Phrases(bigram[tokenized_train])\n",
    "trigram = Phraser(tg_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models.doc2vec import LabeledSentence, Doc2Vec, TaggedDocument\n",
    "\n",
    "def labelize_tweets_bg(tweets,label):\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i, t in enumerate(tweets):\n",
    "        result.append(LabeledSentence(trigram[bigram[t.split()]], [prefix + '_%s' % i]))\n",
    "    return result\n",
    "  \n",
    "#all_x = pd.concat([X_train,X_test])\n",
    "all_x_w2v_bg = labelize_tweets_bg(X, 'all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import utils\n",
    "from tqdm import tqdm\n",
    "\n",
    "#cores = multiprocessing.cpu_count()\n",
    "model_bg_dbow = Doc2Vec(dm=0, size=100, negative=5, min_count=2, alpha=0.065, min_alpha=0.065)\n",
    "model_bg_dbow.build_vocab([x for x in tqdm(all_x_w2v_bg)])\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_bg_dbow.train(utils.shuffle([x for x in tqdm(all_x_w2v_bg)]), total_examples=len(all_x_w2v_bg), epochs=1)\n",
    "    model_bg_dbow.alpha -= 0.002\n",
    "    model_bg_dbow.min_alpha = model_bg_dbow.alpha\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus, size):\n",
    "    vecs = np.zeros((len(corpus), size))\n",
    "    n = 0\n",
    "    for i in range(len(corpus)):\n",
    "        prefix = 'all_' + str(i)\n",
    "        vecs[n] = model.docvecs[prefix]\n",
    "        n += 1\n",
    "    return vecs\n",
    "\n",
    "train_vecs_dbow_bg = get_vectors(model_bg_dbow, X_train, 100)\n",
    "validation_vecs_dbow_bg = get_vectors(model_bg_dbow, X_test, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow_bg, y_train)\n",
    "clf.score(validation_vecs_dbow_bg, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
